{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sales = 0 + quantity = 0 ; returned/cancelled\n",
    "2. sales = 0 quantity = value ; shipped\n",
    "3. order id not has unique value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py:151\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomputation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    135\u001b[39m     concat,\n\u001b[32m    136\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     qcut,\n\u001b[32m    149\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\api\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     extensions,\n\u001b[32m      4\u001b[39m     indexers,\n\u001b[32m      5\u001b[39m     interchange,\n\u001b[32m      6\u001b[39m     types,\n\u001b[32m      7\u001b[39m     typing,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minterchange\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mextensions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtyping\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwindow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Expanding,\n\u001b[32m     21\u001b[39m     ExpandingGroupby,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     Window,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[32m     34\u001b[39m __all__ = [\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDataFrameGroupBy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDatetimeIndexResamplerGroupby\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWindow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     read_json,\n\u001b[32m      3\u001b[39m     to_json,\n\u001b[32m      4\u001b[39m     ujson_dumps,\n\u001b[32m      5\u001b[39m     ujson_loads,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_table_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[32m      9\u001b[39m __all__ = [\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mujson_dumps\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mujson_loads\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbuild_table_schema\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_normalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_table_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     build_table_schema,\n\u001b[32m     69\u001b[39m     parse_table_schema,\n\u001b[32m     70\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     75\u001b[39m         Hashable,\n\u001b[32m     76\u001b[39m         Mapping,\n\u001b[32m     77\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     TextFileReader,\n\u001b[32m      3\u001b[39m     TextParser,\n\u001b[32m      4\u001b[39m     read_csv,\n\u001b[32m      5\u001b[39m     read_fwf,\n\u001b[32m      6\u001b[39m     read_table,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mTextFileReader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTextParser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mread_csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mread_fwf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mread_table\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     AbstractMethodError,\n\u001b[32m     35\u001b[39m     ParserWarning,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:1418\u001b[39m, in \u001b[36minit pandas._libs.parsers\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_15092\\1076197515.py:1: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  a=pd.read_csv(\"Amazon Sale Report.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Order_Status</th>\n",
       "      <th>Fulfillment_Type</th>\n",
       "      <th>Sales_Channel</th>\n",
       "      <th>Shipping_Service_Level</th>\n",
       "      <th>Product_Style</th>\n",
       "      <th>Product_SKU</th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Sale_Amount</th>\n",
       "      <th>Shipping_City</th>\n",
       "      <th>Shipping_State</th>\n",
       "      <th>Shipping_Postal_Code</th>\n",
       "      <th>Shipping_Country</th>\n",
       "      <th>Business_to_Business</th>\n",
       "      <th>Fulfilled_By</th>\n",
       "      <th>Promotion_IDs</th>\n",
       "      <th>Promotion_ID_Count</th>\n",
       "      <th>avg. value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69617</td>\n",
       "      <td>171-5348080-7305151</td>\n",
       "      <td>5/15/2022</td>\n",
       "      <td>Shipped - Delivered to Buyer</td>\n",
       "      <td>Merchant</td>\n",
       "      <td>Amazon.in</td>\n",
       "      <td>Standard</td>\n",
       "      <td>SET355</td>\n",
       "      <td>SET355-KR-PP-XS</td>\n",
       "      <td>Set</td>\n",
       "      <td>...</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UTTAR PRADESH</td>\n",
       "      <td>226002.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>False</td>\n",
       "      <td>Easy Ship</td>\n",
       "      <td>Amazon PLCC Free-Financing Universal Merchant ...</td>\n",
       "      <td>17</td>\n",
       "      <td>1229.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             Order_ID Order_Date                  Order_Status  \\\n",
       "0  69617  171-5348080-7305151  5/15/2022  Shipped - Delivered to Buyer   \n",
       "\n",
       "  Fulfillment_Type Sales_Channel Shipping_Service_Level Product_Style  \\\n",
       "0         Merchant     Amazon.in               Standard        SET355   \n",
       "\n",
       "       Product_SKU Product_Category  ... Sale_Amount Shipping_City  \\\n",
       "0  SET355-KR-PP-XS              Set  ...      1229.0           NaN   \n",
       "\n",
       "  Shipping_State  Shipping_Postal_Code Shipping_Country  Business_to_Business  \\\n",
       "0  UTTAR PRADESH              226002.0               IN                 False   \n",
       "\n",
       "  Fulfilled_By                                      Promotion_IDs  \\\n",
       "0    Easy Ship  Amazon PLCC Free-Financing Universal Merchant ...   \n",
       "\n",
       "  Promotion_ID_Count avg. value  \n",
       "0                 17     1229.0  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=pd.read_csv(\"Amazon Sale Report.csv\")\n",
    "a.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Sale_Amount</th>\n",
       "      <th>Promotion_ID_Count</th>\n",
       "      <th>avg. value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>128975.000000</td>\n",
       "      <td>128975.000000</td>\n",
       "      <td>128975.000000</td>\n",
       "      <td>128975.000000</td>\n",
       "      <td>128975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>64487.000000</td>\n",
       "      <td>0.904431</td>\n",
       "      <td>609.363662</td>\n",
       "      <td>5.559543</td>\n",
       "      <td>582.597612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>37232.019822</td>\n",
       "      <td>0.313354</td>\n",
       "      <td>313.347147</td>\n",
       "      <td>9.048845</td>\n",
       "      <td>328.055021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32243.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>413.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>399.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64487.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>583.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>568.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>96730.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>771.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>771.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>128974.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5584.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>2598.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index       Quantity    Sale_Amount  Promotion_ID_Count  \\\n",
       "count  128975.000000  128975.000000  128975.000000       128975.000000   \n",
       "mean    64487.000000       0.904431     609.363662            5.559543   \n",
       "std     37232.019822       0.313354     313.347147            9.048845   \n",
       "min         0.000000       0.000000       0.000000            0.000000   \n",
       "25%     32243.500000       1.000000     413.000000            0.000000   \n",
       "50%     64487.000000       1.000000     583.000000            1.000000   \n",
       "75%     96730.500000       1.000000     771.000000            5.000000   \n",
       "max    128974.000000      15.000000    5584.000000           36.000000   \n",
       "\n",
       "          avg. value  \n",
       "count  128975.000000  \n",
       "mean      582.597612  \n",
       "std       328.055021  \n",
       "min         0.000000  \n",
       "25%       399.000000  \n",
       "50%       568.000000  \n",
       "75%       771.000000  \n",
       "max      2598.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.describe()\n",
    "# a.corr() - than use heat map for corelation purpose. blue clr is high match\n",
    "#scatter plot between two columns to see if there is any correlation beween 2 columns (linear dependecy)\n",
    "#  box plot - also used for to see how column 1 value are changing depending on column 2\n",
    "#  for statistical analysus for particular column - density , box ,histogram\n",
    "# regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fulfillment_Type\n",
       "Amazon      89698\n",
       "Merchant    39277\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['Fulfillment_Type'].value_counts()\n",
    "# /pie chart / bar graph can be drawn column vise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m.loc[a[\u001b[33m'\u001b[39m\u001b[33mShipping_State\u001b[39m\u001b[33m'\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mUTTAR PRADESH\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m#ye sql ki tarah rows show krega function\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# a.loc[] - row filtering like sql\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.loc[a['Shipping_State']=='UTTAR PRADESH']  #ye sql ki tarah rows show krega function\n",
    "# a.loc[] - row filtering like sql\n",
    "# short cut key for below code shell below = b\n",
    "# short cut key for above code shell below = a\n",
    "# short cut key for delete code shell below = dd (2 times d)\n",
    "# short cut key for command mode shell below = escape\n",
    "# short cut key for editting mode shell below = enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Promotion_ID_Count'] = a['Promotion_IDs'].fillna('').apply(lambda x: len([i for i in x.split(',') if i.strip() != '']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sale_Amount  Quantity  avg. value\n",
      "0       647.62         0         inf\n",
      "1       406.00         1       406.0\n",
      "2       329.00         1       329.0\n",
      "3       753.33         0         inf\n",
      "4       574.00         1       574.0\n"
     ]
    }
   ],
   "source": [
    "# Add average value column to DataFrame a\n",
    "a['avg. value'] = a['Sale_Amount'] / a['Quantity']\n",
    "\n",
    "# Verify the new column is added\n",
    "print(a[['Sale_Amount', 'Quantity', 'avg. value']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128975 entries, 0 to 128974\n",
      "Data columns (total 25 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   index                   128975 non-null  int64  \n",
      " 1   Order_ID                128975 non-null  object \n",
      " 2   Order_Date              128975 non-null  object \n",
      " 3   Order_Status            128975 non-null  object \n",
      " 4   Fulfillment_Type        128975 non-null  object \n",
      " 5   Sales_Channel           128975 non-null  object \n",
      " 6   Shipping_Service_Level  128975 non-null  object \n",
      " 7   Product_Style           128975 non-null  object \n",
      " 8   Product_SKU             128975 non-null  object \n",
      " 9   Product_Category        128975 non-null  object \n",
      " 10  Product_Size            128975 non-null  object \n",
      " 11  Amazon_Standard_ID      128975 non-null  object \n",
      " 12  Courier_Status          122103 non-null  object \n",
      " 13  Quantity                128975 non-null  int64  \n",
      " 14  Currency                121180 non-null  object \n",
      " 15  Sale_Amount             121180 non-null  float64\n",
      " 16  Shipping_City           128942 non-null  object \n",
      " 17  Shipping_State          128942 non-null  object \n",
      " 18  Shipping_Postal_Code    128942 non-null  float64\n",
      " 19  Shipping_Country        128942 non-null  object \n",
      " 20  Business_to_Business    128975 non-null  bool   \n",
      " 21  Fulfilled_By            39277 non-null   object \n",
      " 22  Promotion_IDs           79822 non-null   object \n",
      " 23  Promotion_ID_Count      128975 non-null  int64  \n",
      " 24  avg. value              121180 non-null  float64\n",
      "dtypes: bool(1), float64(3), int64(3), object(18)\n",
      "memory usage: 23.7+ MB\n"
     ]
    }
   ],
   "source": [
    "a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 0 values in 'avg. value': 2343\n",
      "Count of string values in 'avg. value': 0\n",
      "Count of missing values (NaN) in 'avg. value': 7795\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter the rows where 'avg. value' is 0, a string, or missing\n",
    "zero_values = a[a['avg. value'] == 0]\n",
    "string_values = a[a['avg. value'].apply(lambda x: isinstance(x, str))]\n",
    "missing_values = a[a['avg. value'].isna()]\n",
    "\n",
    "# Get the counts for each category\n",
    "zero_count = len(zero_values)\n",
    "string_count = len(string_values)\n",
    "missing_count = len(missing_values)\n",
    "\n",
    "# Print the counts for each category\n",
    "print(f\"Count of 0 values in 'avg. value': {zero_count}\")\n",
    "print(f\"Count of string values in 'avg. value': {string_count}\")\n",
    "print(f\"Count of missing values (NaN) in 'avg. value': {missing_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_count =  10138\n",
      "Percentage of rows with missing or zero 'avg. value': 7.86%\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'avg. value' is missing or zero\n",
    "missing_or_zero_avg_value_rows = a[a['avg. value'].isna() | (a['avg. value'] == 0)]\n",
    "\n",
    "# Export to Excel\n",
    "missing_or_zero_avg_value_rows.to_excel('missing_or_zero_avg_value.xlsx', index=False)\n",
    "print( \"total_count = \",len(missing_or_zero_avg_value_rows))\n",
    "\n",
    "total_rows = len(a)\n",
    "\n",
    "# Rows where 'avg. value' is missing or zero (already created)\n",
    "missing_or_zero_count = len(missing_or_zero_avg_value_rows)\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (missing_or_zero_count / total_rows) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f\"Percentage of rows with missing or zero 'avg. value': {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after filling:\n",
      "index                     0\n",
      "Order_ID                  0\n",
      "Order_Date                0\n",
      "Order_Status              0\n",
      "Fulfillment_Type          0\n",
      "Sales_Channel             0\n",
      "Shipping_Service_Level    0\n",
      "Product_Style             0\n",
      "Product_SKU               0\n",
      "Product_Category          0\n",
      "Product_Size              0\n",
      "Amazon_Standard_ID        0\n",
      "Courier_Status            0\n",
      "Quantity                  0\n",
      "Currency                  0\n",
      "Sale_Amount               0\n",
      "Shipping_City             0\n",
      "Shipping_State            0\n",
      "Shipping_Postal_Code      0\n",
      "Shipping_Country          0\n",
      "Business_to_Business      0\n",
      "Fulfilled_By              0\n",
      "Promotion_IDs             0\n",
      "Promotion_ID_Count        0\n",
      "avg. value                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values for each column based on data type\n",
    "a = a.fillna({\n",
    "    'Order_ID': 0,           # String columns\n",
    "    'Order_Date': 0,\n",
    "    'Order_Status': 'Unknown',\n",
    "    'Fulfillment_Type': 'Unknown',\n",
    "    'Sales_Channel': 'Unknown',\n",
    "    'Shipping_Service_Level': 'Unknown',\n",
    "    'Product_Style': 'Unknown',\n",
    "    'Product_SKU': 'Unknown',\n",
    "    'Product_Category': 'Unknown',\n",
    "    'Product_Size': 'Unknown',\n",
    "    'Amazon_Standard_ID': 'Unknown',\n",
    "    'Courier_Status': 'Unknown',\n",
    "    'Quantity': 0,                   # Numeric columns\n",
    "    'Currency': 'Unknown',\n",
    "    'Sale_Amount': 0.0,              # Float columns\n",
    "    'Shipping_City': 'Unknown',\n",
    "    'Shipping_State': 'Unknown',\n",
    "    'Shipping_Postal_Code': 'Unknown',\n",
    "    'Shipping_Country': 'Unknown',\n",
    "    'Fulfilled_By': 'Unknown',\n",
    "    'Promotion_IDs': 'Unknown',\n",
    "    'avg. value' : 0.0\n",
    "})\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(\"Missing values after filling:\")\n",
    "print(a.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 128975 unique values\n",
      "Order_ID: 120378 unique values\n",
      "Order_Date: 91 unique values\n",
      "Order_Status: 13 unique values\n",
      "Fulfillment_Type: 2 unique values\n",
      "Sales_Channel: 2 unique values\n",
      "Shipping_Service_Level: 2 unique values\n",
      "Product_Style: 1377 unique values\n",
      "Product_SKU: 7195 unique values\n",
      "Product_Category: 9 unique values\n",
      "Product_Size: 11 unique values\n",
      "Amazon_Standard_ID: 7190 unique values\n",
      "Courier_Status: 4 unique values\n",
      "Quantity: 10 unique values\n",
      "Currency: 2 unique values\n",
      "Sale_Amount: 1410 unique values\n",
      "Shipping_City: 8956 unique values\n",
      "Shipping_State: 70 unique values\n",
      "Shipping_Postal_Code: 9460 unique values\n",
      "Shipping_Country: 2 unique values\n",
      "Business_to_Business: 2 unique values\n",
      "Fulfilled_By: 2 unique values\n",
      "Promotion_IDs: 5788 unique values\n",
      "Promotion_ID_Count: 37 unique values\n",
      "avg. value: 748 unique values\n"
     ]
    }
   ],
   "source": [
    "# Loop through each column and print the unique count\n",
    "for column in a.columns:\n",
    "    unique_count = a[column].nunique()\n",
    "    print(f\"{column}: {unique_count} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_State: 48\n",
      "Unique values in Shipping_City: 8956\n",
      "Unique values in Shipping_State:\n",
      "ANDAMAN & NICOBAR\n",
      "ANDHRA PRADESH\n",
      "APO\n",
      "AR\n",
      "ARUNACHAL PRADESH\n",
      "ASSAM\n",
      "BIHAR\n",
      "CHANDIGARH\n",
      "CHHATTISGARH\n",
      "DADRA AND NAGAR\n",
      "DELHI\n",
      "GOA\n",
      "GUJARAT\n",
      "HARYANA\n",
      "HIMACHAL PRADESH\n",
      "JAMMU & KASHMIR\n",
      "JHARKHAND\n",
      "KARNATAKA\n",
      "KERALA\n",
      "LADAKH\n",
      "LAKSHADWEEP\n",
      "MADHYA PRADESH\n",
      "MAHARASHTRA\n",
      "MANIPUR\n",
      "MEGHALAYA\n",
      "MIZORAM\n",
      "NAGALAND\n",
      "NEW DELHI\n",
      "NL\n",
      "ODISHA\n",
      "ORISSA\n",
      "PB\n",
      "PONDICHERRY\n",
      "PUDUCHERRY\n",
      "PUNJAB\n",
      "PUNJAB/MOHALI/ZIRAKPUR\n",
      "RAJASTHAN\n",
      "RAJSHTHAN\n",
      "RAJSTHAN\n",
      "RJ\n",
      "SIKKIM\n",
      "TAMIL NADU\n",
      "TELANGANA\n",
      "TRIPURA\n",
      "UNKNOWN\n",
      "UTTAR PRADESH\n",
      "UTTARAKHAND\n",
      "WEST BENGAL\n"
     ]
    }
   ],
   "source": [
    "# Convert all values in 'Shipping_State' to uppercase and strip them\n",
    "a['Shipping_State'] = a['Shipping_State'].str.strip().str.upper()\n",
    "\n",
    "unique_states_count = a['Shipping_State'].nunique()\n",
    "unique_cities_count = a['Shipping_City'].nunique()\n",
    "# Display the counts\n",
    "print(f\"Unique values in Shipping_State: {unique_states_count}\")\n",
    "print(f\"Unique values in Shipping_City: {unique_cities_count}\")\n",
    "\n",
    "print(\"Unique values in Shipping_State:\")\n",
    "for state in sorted(a['Shipping_State'].dropna().unique()):\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_State: 37\n",
      "Unique values in Shipping_City: 8956\n",
      "Unique values in Shipping_State:\n",
      "ANDAMAN & NICOBAR\n",
      "ANDHRA PRADESH\n",
      "ARUNACHAL PRADESH\n",
      "ASSAM\n",
      "BIHAR\n",
      "CHANDIGARH\n",
      "CHHATTISGARH\n",
      "DADRA AND NAGAR HAVELI AND DAMAN AND DIU\n",
      "DELHI\n",
      "GOA\n",
      "GUJARAT\n",
      "HARYANA\n",
      "HIMACHAL PRADESH\n",
      "JAMMU & KASHMIR\n",
      "JHARKHAND\n",
      "KARNATAKA\n",
      "KERALA\n",
      "LADAKH\n",
      "LAKSHADWEEP\n",
      "MADHYA PRADESH\n",
      "MAHARASHTRA\n",
      "MANIPUR\n",
      "MEGHALAYA\n",
      "MIZORAM\n",
      "NAGALAND\n",
      "ODISHA\n",
      "PUDUCHERRY\n",
      "PUNJAB\n",
      "RAJASTHAN\n",
      "SIKKIM\n",
      "TAMIL NADU\n",
      "TELANGANA\n",
      "TRIPURA\n",
      "UNKNOWN\n",
      "UTTAR PRADESH\n",
      "UTTARAKHAND\n",
      "WEST BENGAL\n"
     ]
    }
   ],
   "source": [
    "# State cleaning map\n",
    "state_corrections = {\n",
    "    'RAJSHTHAN': 'RAJASTHAN',\n",
    "    'RAJSTHAN': 'RAJASTHAN',\n",
    "    'RAJASTHAN': 'RAJASTHAN',\n",
    "    'RJ': 'RAJASTHAN',\n",
    "    'PUNJAB/MOHALI/ZIRAKPUR': 'PUNJAB',\n",
    "    'PB': 'PUNJAB',\n",
    "    'NEW DELHI': 'DELHI',\n",
    "    'NL': 'NAGALAND',\n",
    "    'ORISSA': 'ODISHA',\n",
    "    'PONDICHERRY': 'PUDUCHERRY',\n",
    "    'PUDUCHERRY': 'PUDUCHERRY',\n",
    "    'AR': 'ARUNACHAL PRADESH',\n",
    "    'APO': 'ANDHRA PRADESH',\n",
    "    'UNKNOWN': 'UNKNOWN',\n",
    "    'DELHI': 'DELHI',  # Keep as is\n",
    "    'DADRA AND NAGAR': 'DADRA AND NAGAR HAVELI AND DAMAN AND DIU',\n",
    "    'LAKSHADWEEP': 'LAKSHADWEEP',  # Already clean\n",
    "    # Add other mappings if needed\n",
    "}\n",
    "a['Shipping_State'] = a['Shipping_State'].replace(state_corrections)\n",
    "# printing\n",
    "unique_states_count = a['Shipping_State'].nunique()\n",
    "unique_cities_count = a['Shipping_City'].nunique()\n",
    "# Display the counts\n",
    "print(f\"Unique values in Shipping_State: {unique_states_count}\")\n",
    "print(f\"Unique values in Shipping_City: {unique_cities_count}\")\n",
    "for column in a.columns:\n",
    "    if column in ['Shipping_State']:\n",
    "        unique_values = a[column].unique()\n",
    "print(\"Unique values in Shipping_State:\")\n",
    "for state in sorted(a['Shipping_State'].dropna().unique()):\n",
    "    print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Shipping_City'] = a['Shipping_City'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_City1: 7040\n"
     ]
    }
   ],
   "source": [
    "a['Shipping_City1'] = a['Shipping_City'].str.replace(r'^[^A-Z]+|[^A-Z]+$', '', regex=True)\n",
    "\n",
    "unique_cities_count1 = a['Shipping_City1'].nunique()\n",
    "print(f\"Unique values in Shipping_City1: {unique_cities_count1}\")\n",
    "# Export just the two columns to Excel\n",
    "# a[['Shipping_City', 'Shipping_City1']].to_excel('city_comparison.xlsx', index=False)\n",
    "# Filter rows where the cleaned city differs from the original\n",
    "mismatches = a[a['Shipping_City'] != a['Shipping_City1']]\n",
    "\n",
    "# Export to Excel\n",
    "mismatches[['Shipping_City', 'Shipping_City1']].to_excel('city_mismatches.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_City2: 5924\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import geonamescache\n",
    "\n",
    "# Load GeoNames Indian cities\n",
    "gc = geonamescache.GeonamesCache()\n",
    "cities = gc.get_cities()\n",
    "indian_cities = {city['name'].upper(): city['name'].title() for city in cities.values() if city['countrycode'] == 'IN'}\n",
    "a = a.sort_values(by='Shipping_City1')\n",
    "\n",
    "# Function to clean and match each cell\n",
    "def match_indian_city(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # Split by non-letter characters (e.g. space, comma, digit, special char)\n",
    "    tokens = re.split(r'[^A-Za-z]', text.upper())\n",
    "    for token in tokens:\n",
    "        if token in indian_cities:\n",
    "            return indian_cities[token]  # Return proper case\n",
    "    \n",
    "    return text  # Return original if no match\n",
    "\n",
    "# Apply to the 'Shipping_City1' column\n",
    "a['Shipping_City2'] = a['Shipping_City1'].apply(match_indian_city)\n",
    "unique_cities_count2 = a['Shipping_City2'].nunique()\n",
    "print(f\"Unique values in Shipping_City2: {unique_cities_count2}\")\n",
    "\n",
    "def extract_tokens(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = re.split(r'[^A-Za-z]', text.upper())\n",
    "    return [token for token in tokens if token]\n",
    "a['City_Tokens'] = a['Shipping_City1'].apply(extract_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_City2: 4979\n"
     ]
    }
   ],
   "source": [
    "a['Shipping_City2'] = a['Shipping_City2'].str.replace(\n",
    "    r'\\b(east|west|north|south|dist|district|DISTERICT)\\b', '', \n",
    "    case=False, regex=True\n",
    ").str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a['Shipping_City2'] = a['Shipping_City2'].str.replace(r'^[^A-Z]+|[^A-Z]+$', '', regex=True)\n",
    "unique_cities_count3 = a['Shipping_City2'].nunique()\n",
    "print(f\"Unique values in Shipping_City2: {unique_cities_count3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_City2: 4731\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'Shipping_City2' contains a comma\n",
    "mask = a['Shipping_City2'].str.contains(',')\n",
    "\n",
    "# Extract the last part after the last comma\n",
    "a.loc[mask, 'Shipping_City2'] = a.loc[mask, 'Shipping_City2'].str.split(',').str[-1].str.strip()\n",
    "\n",
    "a['Shipping_City2'] = a['Shipping_City2'].str.replace(r'^[^A-Z]+|[^A-Z]+$', '', regex=True)\n",
    "\n",
    "unique_cities_count3 = a['Shipping_City2'].nunique()\n",
    "print(f\"Unique values in Shipping_City2: {unique_cities_count3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in Shipping_City2: 4712\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Clean city names\n",
    "a['Shipping_City2'] = a['Shipping_City2'].str.replace(\n",
    "    r'\\s*\\(?\\b[WESN]\\b\\)?\\s*', ' ', regex=True  # Remove (E), (W), (S), etc.\n",
    ").str.replace(\n",
    "    r'\\s*\\([^)]*\\)', ' ', regex=True  # Remove any full bracketed suffix like (PO), (D.T), etc.\n",
    ").str.replace(\n",
    "    r'\\s+', ' ', regex=True  # Replace multiple spaces with single space\n",
    ").str.strip()  # Strip leading/trailing spaces\n",
    "\n",
    "\n",
    "unique_cities_count3 = a['Shipping_City2'].nunique()\n",
    "print(f\"Unique values in Shipping_City2: {unique_cities_count3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionary values to a DataFrame\n",
    "# df_cities = pd.DataFrame(indian_cities.values(), columns=['Indian City'])\n",
    "\n",
    "# Export to Excel\n",
    "# df_cities.to_excel('indian_cities.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for export\n",
    "export_df = a[['Shipping_City', 'Shipping_City1', 'Shipping_City2','City_Tokens']].rename(columns={\n",
    "    'Shipping_City': 'Original City',\n",
    "    'Shipping_City1': 'Cleaned City 1',\n",
    "    'Shipping_City2': 'Cleaned City 2',\n",
    "    'City_Tokens' : 'City_Tokens'\n",
    "})\n",
    "\n",
    "# # Export to Excel with custom column titles\n",
    "# export_df.to_excel('city_mismatches.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where 'Shipping_City2' is unique\n",
    "unique_rows = a[a['Shipping_City2'].duplicated(keep=False) == False]\n",
    "\n",
    "# Rename columns for export\n",
    "export_df = unique_rows[['Shipping_City', 'Shipping_City1', 'Shipping_City2','City_Tokens']].rename(columns={\n",
    "    'Shipping_City': 'Original City',\n",
    "    'Shipping_City1': 'Cleaned City 1',\n",
    "    'Shipping_City2': 'Cleaned City 2',\n",
    "    'City_Tokens' : 'City_Tokens'\n",
    "})\n",
    "\n",
    "# Export to Excel\n",
    "# export_df.to_excel('cleaned_shipping.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 128975 entries, 0 to 128974\n",
      "Data columns (total 25 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   index                   128975 non-null  int64  \n",
      " 1   Order_ID                128975 non-null  object \n",
      " 2   Order_Date              128975 non-null  object \n",
      " 3   Order_Status            128975 non-null  object \n",
      " 4   Fulfillment_Type        128975 non-null  object \n",
      " 5   Sales_Channel           128975 non-null  object \n",
      " 6   Shipping_Service_Level  128975 non-null  object \n",
      " 7   Product_Style           128975 non-null  object \n",
      " 8   Product_SKU             128975 non-null  object \n",
      " 9   Product_Category        128975 non-null  object \n",
      " 10  Product_Size            128975 non-null  object \n",
      " 11  Amazon_Standard_ID      128975 non-null  object \n",
      " 12  Courier_Status          128975 non-null  object \n",
      " 13  Quantity                128975 non-null  int64  \n",
      " 14  Currency                128975 non-null  object \n",
      " 15  Sale_Amount             128975 non-null  float64\n",
      " 16  Shipping_City           121127 non-null  object \n",
      " 17  Shipping_State          128975 non-null  object \n",
      " 18  Shipping_Postal_Code    128975 non-null  object \n",
      " 19  Shipping_Country        128975 non-null  object \n",
      " 20  Business_to_Business    128975 non-null  bool   \n",
      " 21  Fulfilled_By            128975 non-null  object \n",
      " 22  Promotion_IDs           128975 non-null  object \n",
      " 23  Promotion_ID_Count      128975 non-null  int64  \n",
      " 24  avg. value              128975 non-null  float64\n",
      "dtypes: bool(1), float64(2), int64(3), object(19)\n",
      "memory usage: 23.7+ MB\n"
     ]
    }
   ],
   "source": [
    "a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Shipping_City']=a['Shipping_City2']\n",
    "a.drop(columns=['Shipping_City1','Shipping_City2','City_Tokens'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 128975 entries, 69617 to 126572\n",
      "Data columns (total 25 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   index                   128975 non-null  int64  \n",
      " 1   Order_ID                128975 non-null  object \n",
      " 2   Order_Date              128975 non-null  object \n",
      " 3   Order_Status            128975 non-null  object \n",
      " 4   Fulfillment_Type        128975 non-null  object \n",
      " 5   Sales_Channel           128975 non-null  object \n",
      " 6   Shipping_Service_Level  128975 non-null  object \n",
      " 7   Product_Style           128975 non-null  object \n",
      " 8   Product_SKU             128975 non-null  object \n",
      " 9   Product_Category        128975 non-null  object \n",
      " 10  Product_Size            128975 non-null  object \n",
      " 11  Amazon_Standard_ID      128975 non-null  object \n",
      " 12  Courier_Status          128975 non-null  object \n",
      " 13  Quantity                128975 non-null  int64  \n",
      " 14  Currency                128975 non-null  object \n",
      " 15  Sale_Amount             128975 non-null  float64\n",
      " 16  Shipping_City           128975 non-null  object \n",
      " 17  Shipping_State          128975 non-null  object \n",
      " 18  Shipping_Postal_Code    128975 non-null  object \n",
      " 19  Shipping_Country        128975 non-null  object \n",
      " 20  Business_to_Business    128975 non-null  bool   \n",
      " 21  Fulfilled_By            128975 non-null  object \n",
      " 22  Promotion_IDs           128975 non-null  object \n",
      " 23  Promotion_ID_Count      128975 non-null  int64  \n",
      " 24  avg. value              128975 non-null  float64\n",
      "dtypes: bool(1), float64(2), int64(3), object(19)\n",
      "memory usage: 24.7+ MB\n"
     ]
    }
   ],
   "source": [
    "a.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_22208\\967058609.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  a[\"avg. value\"].replace([float('inf'), float('-inf')], 0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "a[\"avg. value\"].replace([float('inf'), float('-inf')], 0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('Amazon Sale Report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_excel(\"Amazon Sale Report.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new file Amazon csv code ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_1632\\2861505238.py:1: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  b=pd.read_csv(\"Amazon Sale Report.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Order_Date</th>\n",
       "      <th>Order_Status</th>\n",
       "      <th>Fulfillment_Type</th>\n",
       "      <th>Sales_Channel</th>\n",
       "      <th>Shipping_Service_Level</th>\n",
       "      <th>Product_Style</th>\n",
       "      <th>Product_SKU</th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>...</th>\n",
       "      <th>Sale_Amount</th>\n",
       "      <th>Shipping_City</th>\n",
       "      <th>Shipping_State</th>\n",
       "      <th>Shipping_Postal_Code</th>\n",
       "      <th>Shipping_Country</th>\n",
       "      <th>Business_to_Business</th>\n",
       "      <th>Fulfilled_By</th>\n",
       "      <th>Promotion_IDs</th>\n",
       "      <th>Promotion_ID_Count</th>\n",
       "      <th>avg. value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69617</td>\n",
       "      <td>171-5348080-7305151</td>\n",
       "      <td>5/15/2022</td>\n",
       "      <td>Shipped - Delivered to Buyer</td>\n",
       "      <td>Merchant</td>\n",
       "      <td>Amazon.in</td>\n",
       "      <td>Standard</td>\n",
       "      <td>SET355</td>\n",
       "      <td>SET355-KR-PP-XS</td>\n",
       "      <td>Set</td>\n",
       "      <td>...</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UTTAR PRADESH</td>\n",
       "      <td>226002.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>False</td>\n",
       "      <td>Easy Ship</td>\n",
       "      <td>Amazon PLCC Free-Financing Universal Merchant ...</td>\n",
       "      <td>17</td>\n",
       "      <td>1229.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index             Order_ID Order_Date                  Order_Status  \\\n",
       "0  69617  171-5348080-7305151  5/15/2022  Shipped - Delivered to Buyer   \n",
       "\n",
       "  Fulfillment_Type Sales_Channel Shipping_Service_Level Product_Style  \\\n",
       "0         Merchant     Amazon.in               Standard        SET355   \n",
       "\n",
       "       Product_SKU Product_Category  ... Sale_Amount Shipping_City  \\\n",
       "0  SET355-KR-PP-XS              Set  ...      1229.0           NaN   \n",
       "\n",
       "  Shipping_State  Shipping_Postal_Code Shipping_Country  Business_to_Business  \\\n",
       "0  UTTAR PRADESH              226002.0               IN                 False   \n",
       "\n",
       "  Fulfilled_By                                      Promotion_IDs  \\\n",
       "0    Easy Ship  Amazon PLCC Free-Financing Universal Merchant ...   \n",
       "\n",
       "  Promotion_ID_Count avg. value  \n",
       "0                 17     1229.0  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=pd.read_csv(\"Amazon Sale Report.csv\")\n",
    "b.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "# Convert to float and round to nearest integer before converting to Int64\n",
    "b['Sale_Amount'] = pd.to_numeric(b['Sale_Amount'], errors='coerce')  # Convert to float first\n",
    "b['Sale_Amount'] = b['Sale_Amount'].round().astype('Int64')  # Round and convert to integer\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(\"\\nNumber of NaN values:\", b['Sale_Amount'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing zero sales: 128975\n",
      "Number of rows after removing zero sales: 118837\n",
      "Percentage of rows removed: 0.00%\n",
      "Minimum Sale Amount after cleaning: 199\n"
     ]
    }
   ],
   "source": [
    "# Display count before removing rows\n",
    "print(f\"Number of rows before removing zero sales: {len(b)}\")\n",
    "\n",
    "# Remove rows where Sale_Amount is 0 \n",
    "b = b[b['Sale_Amount'] != 0]\n",
    "\n",
    "# Display count after removing rows\n",
    "print(f\"Number of rows after removing zero sales: {len(b)}\")\n",
    "\n",
    "# Calculate percentage of rows removed\n",
    "percent_removed = ((len(b) - len(b[b['Sale_Amount'] != 0])) / len(b)) * 100\n",
    "print(f\"Percentage of rows removed: {percent_removed:.2f}%\")\n",
    "\n",
    "# Verify the change by checking min sale amount\n",
    "print(f\"Minimum Sale Amount after cleaning: {b['Sale_Amount'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing unknown postal codes: 118806\n",
      "Number of rows after removing unknown postal codes: 118806\n",
      "Percentage of rows removed: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Display count before removing rows\n",
    "print(f\"Number of rows before removing unknown postal codes: {len(b)}\")\n",
    "\n",
    "# Remove rows where Shipping_Postal_Code is 'Unknown'\n",
    "b = b[b['Shipping_Postal_Code'] != 'Unknown']\n",
    "\n",
    "# Display count after removing rows\n",
    "print(f\"Number of rows after removing unknown postal codes: {len(b)}\")\n",
    "\n",
    "# Calculate percentage of rows removed\n",
    "total_removed = len(b) - len(b[b['Shipping_Postal_Code'] != 'Unknown'])\n",
    "percent_removed = (total_removed / len(b)) * 100\n",
    "print(f\"Percentage of rows removed: {percent_removed:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current postal codes sample:\n",
      "0    226002\n",
      "Name: Shipping_Postal_Code, dtype: Int64\n",
      "\n",
      "After conversion:\n",
      "0    226002\n",
      "Name: Shipping_Postal_Code, dtype: Int64\n",
      "\n",
      "Number of NaN values: 0\n"
     ]
    }
   ],
   "source": [
    "# First show sample of current values\n",
    "print(\"Current postal codes sample:\")\n",
    "print(b['Shipping_Postal_Code'].head(1))\n",
    "\n",
    "# Convert directly to numeric since the values are already clean numbers\n",
    "b['Shipping_Postal_Code'] = pd.to_numeric(b['Shipping_Postal_Code'], errors='coerce')\n",
    "\n",
    "# Convert to Int64\n",
    "b['Shipping_Postal_Code'] = b['Shipping_Postal_Code'].astype('Int64')\n",
    "\n",
    "# Verify the conversion\n",
    "print(\"\\nAfter conversion:\")\n",
    "print(b['Shipping_Postal_Code'].head(1))\n",
    "\n",
    "# Check for any NaN values\n",
    "nan_count = b['Shipping_Postal_Code'].isna().sum()\n",
    "print(f\"\\nNumber of NaN values: {nan_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in Order_Status:\n",
      "----------------------------------------\n",
      "Cancelled: 10761\n",
      "Pending: 639\n",
      "Pending - Waiting for Pick Up: 272\n",
      "Shipped: 76062\n",
      "Shipped - Damaged: 1\n",
      "Shipped - Delivered to Buyer: 28038\n",
      "Shipped - Lost in Transit: 3\n",
      "Shipped - Out for Delivery: 35\n",
      "Shipped - Picked Up: 945\n",
      "Shipped - Rejected by Buyer: 11\n",
      "Shipped - Returned to Seller: 1896\n",
      "Shipped - Returning to Seller: 143\n",
      "Total unique values: 12\n",
      "\n",
      "\n",
      "Unique values in Fulfillment_Type:\n",
      "----------------------------------------\n",
      "Amazon: 82094\n",
      "Merchant: 36712\n",
      "Total unique values: 2\n",
      "\n",
      "\n",
      "Unique values in Shipping_Service_Level:\n",
      "----------------------------------------\n",
      "Expedited: 82033\n",
      "Standard: 36773\n",
      "Total unique values: 2\n",
      "\n",
      "\n",
      "Unique values in Product_Category:\n",
      "----------------------------------------\n",
      "Blouse: 859\n",
      "Bottom: 408\n",
      "Dupatta: 3\n",
      "Ethnic Dress: 1061\n",
      "Saree: 154\n",
      "Set: 46018\n",
      "Top: 9989\n",
      "Western Dress: 14472\n",
      "kurta: 45842\n",
      "Total unique values: 9\n",
      "\n",
      "\n",
      "Unique values in Business_to_Business:\n",
      "----------------------------------------\n",
      "False: 117963\n",
      "True: 843\n",
      "Total unique values: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns to check\n",
    "columns_to_check = [\n",
    "    'Order_Status', \n",
    "    'Fulfillment_Type',\n",
    "    'Shipping_Service_Level',\n",
    "    'Product_Category',\n",
    "    'Business_to_Business'\n",
    "]\n",
    "\n",
    "# Print unique values for each column\n",
    "for column in columns_to_check:\n",
    "    print(f\"\\nUnique values in {column}:\")\n",
    "    print(\"-\" * 40)\n",
    "    unique_values = b[column].unique()\n",
    "    for value in sorted(unique_values):\n",
    "        count = b[column].value_counts()[value]\n",
    "        print(f\"{value}: {count}\")\n",
    "    print(f\"Total unique values: {len(unique_values)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    'index', 'Fulfilled_By','Sales_Channel',\n",
    "    'Shipping_Country','Courier_Status','Currency','Promotion_IDs'\n",
    "]\n",
    "b = b.drop(columns=columns_to_remove, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated column types:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 118806 entries, 0 to 128974\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   Order_ID            118806 non-null  object \n",
      " 1   date                118806 non-null  object \n",
      " 2   Order_Status        118806 non-null  object \n",
      " 3   Fulfillment_Type    118806 non-null  object \n",
      " 4   shipping_level      118806 non-null  object \n",
      " 5   style               118806 non-null  object \n",
      " 6   sku                 118806 non-null  object \n",
      " 7   category            118806 non-null  object \n",
      " 8   size                118806 non-null  object \n",
      " 9   Amazon_Standard_ID  118806 non-null  object \n",
      " 10  quantity            118806 non-null  int64  \n",
      " 11  sale                118806 non-null  Int64  \n",
      " 12  city                118806 non-null  object \n",
      " 13  state               118806 non-null  object \n",
      " 14  postal_code         118806 non-null  object \n",
      " 15  b2b                 118806 non-null  bool   \n",
      " 16  Promotion_ID_Count  118806 non-null  int64  \n",
      " 17  avg. value          118806 non-null  float64\n",
      "dtypes: Int64(1), bool(1), float64(1), int64(2), object(13)\n",
      "memory usage: 16.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "b = b.rename(columns={\n",
    "    'Order_Date': 'date',\n",
    "    'Shipping_Postal_Code': 'postal_code',\n",
    "    'Business_to_Business': 'b2b',\n",
    "    'Shipping_State': 'state',\n",
    "    'Shipping_City': 'city',\n",
    "    'Sale_Amount': 'sale',\n",
    "    'Product_Size': 'size',\n",
    "    'Product_Category': 'category',\n",
    "    'Product_SKU': 'sku',\n",
    "    'Product_Style': 'style',\n",
    "    'Quantity': 'quantity',\n",
    "    'Shipping_Service_Level': 'shipping_level',\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_1632\\164755989.py:2: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  b['date'] = pd.to_datetime(b['date'], errors='coerce', infer_datetime_format=True, dayfirst=False)\n",
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_1632\\164755989.py:2: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  b['date'] = pd.to_datetime(b['date'], errors='coerce', infer_datetime_format=True, dayfirst=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    15-05-2022\n",
      "1    01-06-2022\n",
      "2    11-06-2022\n",
      "3    25-05-2022\n",
      "4    15-06-2022\n",
      "Name: date, dtype: object\n",
      "object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_1632\\164755989.py:10: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  b['date'] = pd.to_datetime(b['date'], errors='coerce', infer_datetime_format=True, dayfirst=False)\n",
      "C:\\Users\\dishi\\AppData\\Local\\Temp\\ipykernel_1632\\164755989.py:10: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  b['date'] = pd.to_datetime(b['date'], errors='coerce', infer_datetime_format=True, dayfirst=False)\n"
     ]
    }
   ],
   "source": [
    "# Convert to datetime safely\n",
    "b['date'] = pd.to_datetime(b['date'], errors='coerce', infer_datetime_format=True, dayfirst=False)\n",
    "\n",
    "# Format as dd-mm-yyyy (string)\n",
    "b['date'] = b['date'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "# Confirm\n",
    "print(b['date'].head())\n",
    "print(b.dtypes['date'])\n",
    "b['date'] = pd.to_datetime(b['date'], errors='coerce', infer_datetime_format=True, dayfirst=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated column types:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 118806 entries, 0 to 128974\n",
      "Data columns (total 18 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   Order_ID            118806 non-null  string        \n",
      " 1   date                118805 non-null  datetime64[ns]\n",
      " 2   Order_Status        118806 non-null  string        \n",
      " 3   Fulfillment_Type    118806 non-null  string        \n",
      " 4   shipping_level      118806 non-null  string        \n",
      " 5   style               118806 non-null  string        \n",
      " 6   sku                 118806 non-null  string        \n",
      " 7   category            118806 non-null  string        \n",
      " 8   size                118806 non-null  string        \n",
      " 9   Amazon_Standard_ID  118806 non-null  string        \n",
      " 10  quantity            118806 non-null  int64         \n",
      " 11  sale                118806 non-null  Int64         \n",
      " 12  city                118806 non-null  string        \n",
      " 13  state               118806 non-null  string        \n",
      " 14  postal_code         118806 non-null  string        \n",
      " 15  b2b                 118806 non-null  bool          \n",
      " 16  Promotion_ID_Count  118806 non-null  int64         \n",
      " 17  avg. value          118806 non-null  float64       \n",
      "dtypes: Int64(1), bool(1), datetime64[ns](1), float64(1), int64(2), string(12)\n",
      "memory usage: 16.5 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Convert columns to their appropriate types\n",
    "b = b.astype({\n",
    "    'Order_ID': 'string',\n",
    "    'Order_Status': 'string',\n",
    "    'Fulfillment_Type': 'string',\n",
    "    'shipping_level': 'string',\n",
    "    'style': 'string',\n",
    "    'sku': 'string',\n",
    "    'category': 'string',\n",
    "    'size': 'string',\n",
    "    'Amazon_Standard_ID': 'string',\n",
    "    'city': 'string',\n",
    "    'state': 'string',\n",
    "    'postal_code': 'string',\n",
    "    })\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nUpdated column types:\")\n",
    "print(b.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv('new_amazon_national_sales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################   GRAPHS  ###################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
